
# Load required libraries
from pyspark.sql.functions import col, collect_set
from pyspark.ml.fpm import FPGrowth

# Load CSV files (Adjust paths based on your upload location)
departments = spark.read.format("csv").option("header", "true").load("dbfs:/FileStore/shared_uploads/bhavani6shashi@gmail.com/Data_set_bigdata/departments.csv")
aisles = spark.read.format("csv").option("header", "true").load("dbfs:/FileStore/shared_uploads/bhavani6shashi@gmail.com/Data_set_bigdata/aisles.csv")
orders = spark.read.format("csv").option("header", "true").load("dbfs:/FileStore/shared_uploads/bhavani6shashi@gmail.com/Data_set_bigdata/orders.csv")
order_products = spark.read.format("csv").option("header", "true").load("dbfs:/FileStore/shared_uploads/bhavani6shashi@gmail.com/Data_set_bigdata/order_products__train.csv")
products = spark.read.format("csv").option("header", "true").load("dbfs:/FileStore/shared_uploads/bhavani6shashi@gmail.com/Data_set_bigdata/products.csv")
order_products__prior = spark.read.format("csv").option("header", "true").load("dbfs:/FileStore/shared_uploads/bhavani6shashi@gmail.com/Data_set_bigdata/order_products__prior.csv")

# Join product metadata
product_info = products.join(aisles, 'aisle_id').join(departments, 'department_id')

# Join order_products with product_info
order_details = order_products.join(product_info, 'product_id')

# Join with orders to get full context
full_orders = order_details.join(orders, 'order_id')

# Check schema
full_orders.select("user_id", "order_id", "product_name", "department", "aisle").show(5, truncate=False)


# Prepare transactions by grouping items per order
transactions = full_orders.groupBy("order_id").agg(collect_set("product_name").alias("items"))
transactions.show(5, truncate=False)

# Apply FP-Growth for Market Basket Analysis
fpGrowth = FPGrowth(itemsCol="items", minSupport=0.005, minConfidence=0.2)
model = fpGrowth.fit(transactions)

# Display frequent itemsets
model.freqItemsets.show()

# Display association rules
model.associationRules.show()

# Visualize association rules
display(model.associationRules)

# Save frequent itemsets
from pyspark.sql.functions import concat_ws

# Convert array<string> to comma-separated string
transactions_csv_ready = transactions.withColumn("items", concat_ws(",", "items"))

# Now you can write to CSV
transactions_csv_ready.write.csv("/dbfs/FileStore/transactions.csv", header=True, mode="overwrite")

# Save association rules
itemsets_csv_ready = model.freqItemsets.withColumn("items", concat_ws(",", "items"))
itemsets_csv_ready.write.csv("/dbfs/FileStore/frequent_itemsets.csv", header=True, mode="overwrite")


